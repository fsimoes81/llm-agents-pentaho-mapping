{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/Repos/llm-agents-pentaho-mapping/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/fabio/Repos/llm-agents-pentaho-mapping/.venv/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import duckdb as db\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# from pydantic import BaseModel\n",
    "from typing import Any, Dict, List, Tuple, TypedDict, Annotated\n",
    "from textwrap import dedent\n",
    "# from crewai_tools import tool, FileWriterTool\n",
    "from crewai import Agent, Crew, Process, Task\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatGroq(\n",
    "#     model=\"llama3-8b-8192\",\n",
    "#     temperature = 0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatGoogleGenerativeAI(\n",
    "#     model='gemini-1.5-pro-exp-0801',\n",
    "#     temperature=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_list(file_path: str) -> List:\n",
    "    with open(file_path) as f:\n",
    "        s = f.read()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(folder_path: str):\n",
    "    file_list = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "    file_list.sort()\n",
    "    queries = []  \n",
    "\n",
    "    for sql_file in file_list:\n",
    "        q = query_list(join(folder_path,sql_file))\n",
    "        queries.append({\n",
    "            'query_name': sql_file[:-4],\n",
    "            'sql_code': q \n",
    "        })          \n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sql_files/bn_beneficiario/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_manager = Agent(\n",
    "    role = \"Project Manager\",\n",
    "    goal = \"Ensure that the project has all queries mapped correctly for the success of the migration project.\",\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You're a seasoned Project Manager with over 15 years of experience in data management and IT transformations, \n",
    "        is known for her ability to navigate complex projects with precision and foresight. You began her career as a data analyst, \n",
    "        where developed a deep understanding of SQL and traditional ETL (Extract, Transform, Load) processes. \n",
    "        Over the years, transitioned into project management, where led numerous successful data migration and modernization projects.\n",
    "        You are in charge of a project that aims to transition the company's aging ETL processes, heavily reliant on SQL, \n",
    "        to a modern data stack that leverages ELT (Extract, Load, Transform) processes using PySpark. \n",
    "        The goal is to enhance data processing efficiency, scalability, and maintainability.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    llm=model,\n",
    "    allow_delegation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_analyst = Agent(\n",
    "    role = \"Senior Data Analyst\",\n",
    "    goal = \"Analyze complicate SQL queries and extract the relationship between table name and column name of all tables in queries.\",\n",
    "    backstory=dedent(\n",
    "        \"\"\"\n",
    "        You're a highly specialized developed to dissect and understand complex SQL queries,\n",
    "        you could quickly and accurately extract essential information from intricate SQL statements, \n",
    "        including that ones that has many sub queries.\n",
    "        Your key traits are Analytical prowess, Attention to detail, Vast knowledge of SQL syntax \n",
    "        across multiple database systems.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    llm=model,\n",
    "    allow_delegation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_table_names = Task(\n",
    "    description=dedent(\n",
    "        \"\"\" \n",
    "        Search this SQL query {sql_code} for all table names involved.\n",
    "        It is very important not to ignore any tables. In complex queries, \n",
    "        there are some subqueries that must be observed carefully.\n",
    "        Do it line by line, get all table names and their alias when they are present.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    expected_output=\"List of distinct table names and alias present in the query.\",\n",
    "    agent=project_manager,\n",
    "    allow_delegation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tables_columns = Task(\n",
    "    description=dedent(\n",
    "        \"\"\"\n",
    "        Analyze the SQL query provided below: {sql_code}\n",
    "        Use the list of table names and their aliases extracted in the previous step and find the columns for each of these tables.\n",
    "        For each line of the code do:\n",
    "        1. **Identify Table Names and Aliases**:\n",
    "        Extract all table names along with their aliases used in the query.\n",
    "        2. **Identify Columns and Aliases**:\n",
    "        For each table identified, list all the columns and their corresponding aliases (if any). If no alias is provided, leave the alias field blank.\n",
    "        3. **Output Format**:\n",
    "        Present the information in the following format:\n",
    "        table_name;alias;column_name;column_alias \n",
    "        table1;alias1;columnName_n1;columnAlias_n1 \n",
    "        table1;alias1;columnName_n2;columnAlias_n2 \n",
    "        table2;alias2;columnName_n1;columnAlias_n1\n",
    "\n",
    "        4. **Special Instructions**:\n",
    "        - If a column does not have an alias, repeat the column name in the `column_alias` field.\n",
    "        - Ensure all extracted data adheres strictly to the format specified.        \n",
    "        \"\"\"\n",
    "    ),\n",
    "    expected_output=\"Formated CSV file.\",\n",
    "    agent=sql_analyst,\n",
    "    context=[get_table_names]\n",
    "    #callback=lambda result: result_collector.add_result(result)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = Task(\n",
    "    description=dedent(\n",
    "        \"\"\"\n",
    "        Analyze the SQL query provided: {sql_code}\n",
    "        Use the csv file generated in the previous step and and revise if all tables and columnas were extracted,\n",
    "        if not include the tables and column names that are missing.\n",
    "\n",
    "        **Output Format**:\n",
    "        Present the information in the following format:\n",
    "        table_name;alias;column_name;column_alias \n",
    "        table1;alias1;columnName_n1;columnAlias_n1 \n",
    "        table1;alias1;columnName_n2;columnAlias_n2 \n",
    "        table2;alias2;columnName_n1;columnAlias_n1        \n",
    "\n",
    "        \"\"\"\n",
    "    ),\n",
    "    expected_output=\"Formated CSV file.\",\n",
    "    agent=project_manager,\n",
    "    context=[extract_tables_columns]\n",
    "    #callback=lambda result: result_collector.add_result(result)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rules = Task(\n",
    "    description=dedent(\n",
    "        \"\"\"\n",
    "        Analyze the SQL query provided: {sql_code}\n",
    "        Use the csv file generated in the previous step and find and extract following points:\n",
    "        \n",
    "        1. **Identify SQL Functions and Concatenations**:\n",
    "        Locate all instances of SQL functions (e.g., `NVL`, `DECODE`, `CASE`, `SUM`, etc.) or concatenations used in the query. Refer to these as \"rules.\"\n",
    "\n",
    "        2. **Extract Corresponding Aliases**:\n",
    "        For each rule identified, find the corresponding alias. The alias is the final name given to the column in the query.\n",
    "\n",
    "        3. **Extract Column Names**:\n",
    "        Identify the column name associated with each rule and alias.\n",
    "\n",
    "        4. **Output Format**:\n",
    "        Present the information in the following format:\n",
    "        column_name;alias;rule \n",
    "        columnName_n1;alias_n1;\"rule_n1\" \n",
    "        columnName_n2;alias_n2;\"rule_n2\" \n",
    "        columnName_n3;alias_n3;\"rule_n3\"\n",
    "\n",
    "        - **Note**: Only include columns where a rule is found.\n",
    "        - If any element (e.g., alias) is missing, indicate it with `NULL`.\n",
    "\n",
    "        5. **Final Instructions**:\n",
    "        Ensure that all extracted data adheres strictly to the format specified.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    expected_output=\"Formated CSV file.\",\n",
    "    agent=sql_analyst,\n",
    "    context=[extract_tables_columns]\n",
    "    #callback=lambda result: result_collector.add_result(result)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents = [project_manager,sql_analyst],\n",
    "    tasks = [get_table_names, extract_tables_columns, revision, extract_rules],\n",
    "    process = Process.sequential,\n",
    "    verbose = 0,\n",
    "    memory=False,\n",
    "    output_log_file=\"crew.log\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(data, filename, header: list):\n",
    "    # Split the string by newlines to get rows\n",
    "    rows = data.split('\\n')\n",
    "    \n",
    "    # Split each row by semicolons to get columns\n",
    "    formatted_data = [row.split(';') for row in rows]\n",
    "    \n",
    "    # Output filename\n",
    "    output_filename = f\"{'output'}/{filename}.parquet\"\n",
    "\n",
    "    df = pd.DataFrame(formatted_data, columns=header)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table,output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename, header: list):\n",
    "    # Split the string by newlines to get rows\n",
    "    rows = data.split('\\n')\n",
    "    \n",
    "    # Split each row by semicolons to get columns\n",
    "    formatted_data = [row.split(';') for row in rows]\n",
    "    \n",
    "    # Output filename\n",
    "    output_filename = f\"{'output'}/{filename}.csv\"\n",
    "\n",
    "    # Write the formatted data to a CSV file\n",
    "    with open(output_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, delimiter=',',quotechar =',',quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(header)  # Write header\n",
    "        writer.writerows(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_beneficiario\n",
      "02_sam_familia_teto_pf\n",
      "03_1_busca_microsiga\n",
      "03_2_sem_setor\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(query['query_name'])\n",
    "    file_name = query['query_name']\n",
    "    result = crew.kickoff(inputs=query)\n",
    "    task_output = revision.output\n",
    "    save_to_csv(task_output.raw, file_name, ['table_name','alias','column_name','column_alias'])\n",
    "    task_output = extract_rules.output\n",
    "    save_to_csv(task_output.raw, file_name+'_rules', ['column_name','alias','rule'])\n",
    "\n",
    "    #result_collector.add_result('export_tables_columns',output.raw,file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processa CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────┬─────────┬─────────────────────┬─────────────────────┐\n",
       "│    table_name    │  alias  │     column_name     │    column_alias     │\n",
       "│     varchar      │ varchar │       varchar       │       varchar       │\n",
       "├──────────────────┼─────────┼─────────────────────┼─────────────────────┤\n",
       "│ SAM_BENEFICIARIO │ BEN     │ HANDLE              │ HANDLE              │\n",
       "│ SAM_BENEFICIARIO │ BEN     │ ENDERECORESIDENCIAL │ ENDERECORESIDENCIAL │\n",
       "│ SAM_ENDERECO     │ ENDR    │ DDD1                │ DDD1                │\n",
       "│ SAM_ENDERECO     │ ENDR    │ PREFIXO1            │ PREFIXO1            │\n",
       "│ SAM_ENDERECO     │ ENDR    │ NUMERO1             │ NUMERO1             │\n",
       "│ SAM_ENDERECO     │ ENDR    │ HANDLE              │ HANDLE              │\n",
       "│ SAM_BENEFICIARIO │ NULL    │ SETOR_UNIMED        │ SETOR_UNIMED        │\n",
       "└──────────────────┴─────────┴─────────────────────┴─────────────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql(\n",
    "\"\"\"     \n",
    "    select \n",
    "        *\n",
    "    from 'output/03_2_sem_setor.csv'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.sql(\n",
    "\"\"\"     \n",
    "    with ben_ori as (\n",
    "        select 'beneficiario' as origem, table_name, alias as table_alias, column_name from 'output/01_beneficiario.csv' \n",
    "        union all\n",
    "        select 'sam_familia_teto_pf' as origem, table_name, alias as table_alias, column_name from 'output/02_sam_familia_teto_pf.csv' \n",
    "        union all\n",
    "        select 'busca_microsiga' as origem, table_name, alias as table_alias, column_name from 'output/03_1_busca_microsiga.csv'\n",
    "        union all\n",
    "        select 'sem_setor' as origem, table_name, alias as table_alias, column_name from 'output/03_2_sem_setor.csv'\n",
    "    ),\n",
    "    ben_rul as (\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule\n",
    "        from 'output/01_beneficiario_rules.csv'\n",
    "        union all\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule\n",
    "        from 'output/02_sam_familia_teto_pf_rules.csv' \n",
    "        union all\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule\n",
    "        from 'output/03_1_busca_microsiga_rules.csv'\n",
    "        union all\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule        \n",
    "        from 'output/03_2_sem_setor_rules.csv'\n",
    "    ),\n",
    "    ben_dest as (\n",
    "        select * from 'output/99_bn_beneficiario.csv'\n",
    "    )\n",
    "    select \n",
    "        distinct \n",
    "        ben_dest.*,\n",
    "        ben_ori.*,\n",
    "        ben_rul.*\n",
    "    from ben_dest\n",
    "    left join ben_ori \n",
    "    on(ben_ori.column_name = ben_dest.column_name)\n",
    "    left join ben_rul\n",
    "    on(ben_dest.colum_name_renamed = ben_rul.alias)\n",
    "    order by ben_ori.column_name\n",
    "\"\"\").to_csv('duckdb_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Column \"column_name\" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43;03m\"\"\"     \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43;03m        select \u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43;03m            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43;03m            alias,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43;03m            rule\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43;03m        from 'output/01_beneficiario_rules.csv'\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43;03m        union all\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43;03m        select \u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43;03m            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43;03m            alias,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43;03m            rule\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43;03m        from 'output/02_sam_familia_teto_pf_rules.csv' \u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43;03m        union all\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43;03m        select \u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43;03m            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43;03m            alias,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43;03m            rule\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43;03m        from 'output/03_1_busca_microsiga_rules.csv'\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43;03m        union all\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43;03m        select \u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43;03m            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43;03m            alias,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43;03m            rule        \u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43;03m        from 'output/03_2_sem_setor_rules.csv'\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/llm-agents-pentaho-mapping/.venv/lib/python3.10/site-packages/duckdb/__init__.py:457\u001b[0m, in \u001b[0;36msql\u001b[0;34m(query, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     conn \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:default:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mBinderException\u001b[0m: Binder Error: Column \"column_name\" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined"
     ]
    }
   ],
   "source": [
    "db.sql(\n",
    "\"\"\"     \n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule\n",
    "        from 'output/01_beneficiario_rules.csv'\n",
    "        union all\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule\n",
    "        from 'output/02_sam_familia_teto_pf_rules.csv' \n",
    "        union all\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule\n",
    "        from 'output/03_1_busca_microsiga_rules.csv'\n",
    "        union all\n",
    "        select \n",
    "            SUBSTR(column_name, INSTR(column_name, '.') + 1) AS column_name,\n",
    "            alias,\n",
    "            rule        \n",
    "        from 'output/03_2_sem_setor_rules.csv'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserException",
     "evalue": "Parser Error: syntax error at or near \"table\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect table from \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/01_beneficiario.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/llm-agents-pentaho-mapping/.venv/lib/python3.10/site-packages/duckdb/__init__.py:457\u001b[0m, in \u001b[0;36msql\u001b[0;34m(query, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     conn \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:default:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mParserException\u001b[0m: Parser Error: syntax error at or near \"table\""
     ]
    }
   ],
   "source": [
    "db.sql(\"select table from 'output/01_beneficiario.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
